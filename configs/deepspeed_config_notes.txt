DeepSpeed Configuration Notes
============================

Batch Size Calculation:
- train_micro_batch_size_per_gpu: 32
- gradient_accumulation_steps: 2  
- num_gpus: 8
- Effective batch size per GPU: 32 * 2 = 64
- Total batch size: 64 * 8 = 512

This matches the stage1_wikipedia.yaml configuration:
- per_device_train_batch_size: 64
- gradient_accumulation_steps: 2
- Total with 8 GPUs: 64 * 2 * 8 = 1024

Note: The micro_batch_size_per_gpu (32) is half of per_device_train_batch_size (64)
because DeepSpeed's gradient accumulation doubles it.

For dynamic batch sizes with sequence curriculum:
- Adjust train_micro_batch_size_per_gpu based on sequence length
- Sequence 256: batch_size 128
- Sequence 512: batch_size 96  
- Sequence 1024: batch_size 64
- Sequence 2048: batch_size 32
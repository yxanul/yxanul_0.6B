# Main training configuration for FineWeb-Edu dataset
# Optimized for NVIDIA NGC containers with FP8/BF16 support

stage:
  name: "fineweb_edu_training"
  description: "Training on FineWeb-Edu high-quality dataset"
  
data:
  dataset_name: "Yxanul/fineweb-edu-highest-quality-2025"  # Update when dataset is fixed
  dataset_split: "train"
  streaming: true  # Use streaming to avoid memory issues
  max_sequence_length: 2048
  stride: 1024
  tokenizer: "gpt2"
  
training:
  # Duration
  num_epochs: 1
  max_steps: 150000
  
  # Batch sizes (will be adjusted by curriculum)
  per_device_train_batch_size: 24
  gradient_accumulation_steps: 1
  
  # Learning rate
  learning_rate: 6e-4
  min_learning_rate: 6e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  
  # Curriculum Learning (3-4x speedup)
  use_curriculum: true
  curriculum_stages:
    # Start small for fast initial learning
    - {step: 0,     seq_len: 128,  batch_size: 256, lr_scale: 8.0, grad_clip: 5.0}
    - {step: 3000,  seq_len: 256,  batch_size: 128, lr_scale: 5.0, grad_clip: 2.0}
    - {step: 6000,  seq_len: 512,  batch_size: 64,  lr_scale: 3.0, grad_clip: 1.0}
    - {step: 10000, seq_len: 768,  batch_size: 42,  lr_scale: 2.0, grad_clip: 0.5}
    - {step: 15000, seq_len: 1024, batch_size: 32,  lr_scale: 1.5, grad_clip: 0.4}
    - {step: 25000, seq_len: 1536, batch_size: 21,  lr_scale: 1.0, grad_clip: 0.3}
    - {step: 40000, seq_len: 2048, batch_size: 16,  lr_scale: 0.7, grad_clip: 0.3}
    - {step: 60000, seq_len: 2048, batch_size: 16,  lr_scale: 0.5, grad_clip: 0.3}
  
  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  weight_decay: 0.1
  
  # Mixed precision (NGC container will use FP8 if available, else BF16)
  bf16: true
  fp16: false
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 10000
  save_total_limit: 3
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 2000
  
  # Logging
  logging_steps: 100
  logging_first_step: true
  report_to: ["wandb"]
  
  # Seed
  seed: 42
  
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 32

# Memory estimates for 24GB GPU (RTX 4090)
memory_usage:
  seq_128_batch_256: "~8GB"
  seq_512_batch_64: "~12GB"
  seq_1024_batch_32: "~16GB"
  seq_2048_batch_16: "~20GB"
  
expected_performance:
  tokens_per_second_bf16: "~20,000-25,000"
  tokens_per_second_fp8: "~35,000-40,000"
  time_to_perplexity_100: "~10-12 hours"
  time_to_perplexity_50: "~20-24 hours"
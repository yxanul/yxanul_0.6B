# Ultra-Aggressive Curriculum Learning Configuration
# Leveraging SuperBPE-t80k efficiency (7.184 chars/token)
# Starting from seq_len=8 like Microsoft DeepSpeed

data:
  dataset_name: "fineweb-edu-highest-quality-2025"  # Local dataset
  max_sequence_length: 2048
  tokenizer: "SuperBPE-t80k"  # 37.5% fewer tokens!

training:
  # Total tokens to process (with SuperBPE-t80k)
  # 4.1B text tokens = 2.61B actual tokens
  max_steps: 200000
  save_steps: 10000
  eval_steps: 1000
  logging_steps: 100
  
  # FP8 settings
  use_fp8: true
  fp8_format: "e4m3"
  
  # Ultra-aggressive curriculum (10 stages!)
  use_curriculum: true
  curriculum_stages:
    # Stage 1: Single phrases (8 tokens = ~57 chars)
    # "The quick brown fox jumps" level
    - step: 0
      seq_len: 8
      batch_size: 512  # Increased - we have VRAM headroom
      lr_scale: 0.3      # Very gentle start
      grad_clip: 0.5     # Heavy clipping for stability
      warmup_steps: 1000
      gradient_accumulation_steps: 8  # Effective batch = 4096
      description: "Learning basic phrases and word patterns"
      # 4,096 tokens per batch effective
    
    # Stage 2: Complete sentences (16 tokens = ~115 chars)
    # "The quick brown fox jumps over the lazy dog" level
    - step: 2000
      seq_len: 16
      batch_size: 512  # Increased
      lr_scale: 0.4
      grad_clip: 0.5
      gradient_accumulation_steps: 4  # Effective batch = 2048
      description: "Learning complete sentences and basic grammar"
      # 8,192 tokens per batch effective
    
    # Stage 3: Compound sentences (32 tokens = ~230 chars)
    # Full paragraph with 2-3 sentences
    - step: 5000
      seq_len: 32
      batch_size: 384  # Increased, divisible by 8
      lr_scale: 0.5
      grad_clip: 0.7
      gradient_accumulation_steps: 3  # Effective batch = 1152
      description: "Learning paragraph structure and flow"
      # 12,288 tokens per batch effective
    
    # Stage 4: Short paragraphs (64 tokens = ~460 chars)
    - step: 10000
      seq_len: 64
      batch_size: 192  # Increased, divisible by 8
      lr_scale: 0.6
      grad_clip: 0.8
      gradient_accumulation_steps: 3  # Effective batch = 576
      description: "Learning multi-sentence coherence"
      # 12,288 tokens per batch effective
    
    # Stage 5: Full paragraphs (128 tokens = ~920 chars)
    - step: 20000
      seq_len: 128
      batch_size: 96  # Increased, divisible by 8
      lr_scale: 0.75
      grad_clip: 0.9
      gradient_accumulation_steps: 3  # Effective batch = 288
      description: "Learning extended context and topic development"
      # 65,536 tokens per batch!
    
    # Stage 6: Multiple paragraphs (256 tokens = ~1,840 chars)
    - step: 35000
      seq_len: 256
      batch_size: 48  # Increased, divisible by 8
      lr_scale: 0.85
      grad_clip: 1.0
      gradient_accumulation_steps: 3  # Effective batch = 144
      description: "Learning section-level coherence"
      # 12,288 tokens per batch effective
    
    # Stage 7: Article sections (512 tokens = ~3,680 chars)
    - step: 55000
      seq_len: 512
      batch_size: 24  # Increased, divisible by 8
      lr_scale: 0.95
      grad_clip: 1.0
      gradient_accumulation_steps: 3  # Effective batch = 72
      description: "Learning document structure"
      # 12,288 tokens per batch effective
    
    # Stage 8: Half articles (768 tokens = ~5,520 chars)
    - step: 80000
      seq_len: 768
      batch_size: 16  # Keep conservative for longer sequences
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 3  # Effective batch = 48
      description: "Learning extended reasoning"
      # 12,288 tokens per batch effective
    
    # Stage 9: Near-full articles (1536 tokens = ~11,040 chars)
    - step: 110000
      seq_len: 1536
      batch_size: 8  # Keep conservative for very long sequences
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 6  # Effective batch = 48
      description: "Learning complete document understanding"
      # 12,288 tokens per batch effective
    
    # Stage 10: Full articles (2048 tokens = ~14,720 chars)
    - step: 140000
      seq_len: 2048
      batch_size: 8  # Keep conservative for maximum sequences
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 4  # Effective batch = 32
      description: "Maximum context window training"
      # 16,384 tokens per batch effective

  # Learning rate schedule
  learning_rate: 6e-4  # Higher for smaller sequences
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01
  
  # Gradient accumulation (adjusted per stage automatically)
  gradient_accumulation_steps: 1
  
  # Mixed precision
  mixed_precision: "bf16"
  tf32: true
  
  # Memory optimization
  gradient_checkpointing: true
  use_flash_attention: true

# Validation settings
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 32
  eval_strategy: "steps"
  metric_for_best_model: "perplexity"
  greater_is_better: false

# Logging
logging:
  report_to: ["console", "wandb"]
  wandb_project: "yxanul-ultra-curriculum"
  wandb_run_name: "superbpe-t80k-seq8-start"
  log_level: "info"

# Expected timeline with this ultra-aggressive curriculum:
# 
# Hour 0-0.5: Stages 1-2 (8-16 tokens) - Basic patterns
#   - Processing ~100M+ micro-examples
#   - Learning: word associations, common phrases
#
# Hour 0.5-1: Stages 3-4 (32-64 tokens) - Sentence mastery  
#   - Processing ~50M examples
#   - Learning: grammar, sentence structure
#
# Hour 1-3: Stages 5-6 (128-256 tokens) - Paragraph mastery
#   - Processing ~25M examples  
#   - Learning: coherence, topic development
#
# Hour 3-6: Stages 7-8 (512-768 tokens) - Section mastery
#   - Processing ~10M examples
#   - Learning: document structure, arguments
#
# Hour 6-10: Stages 9-10 (1536-2048 tokens) - Full documents
#   - Processing ~5M examples
#   - Learning: long-range dependencies, complete understanding
#
# Total: ~10 hours to excellent perplexity!
# 
# Why this works with SuperBPE-t80k:
# - 8 tokens = meaningful phrase (vs gibberish with GPT-2)
# - 16 tokens = complete sentence (vs fragment with GPT-2)  
# - Massive batch sizes at short sequences = incredible throughput
# - GPU stays saturated throughout training
# - Natural progression from simple to complex
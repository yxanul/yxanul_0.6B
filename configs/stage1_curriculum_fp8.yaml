# FP8-Optimized Curriculum Learning Configuration
# Takes advantage of FP8's 2x throughput for even larger batches
# Targets 22-23GB VRAM with FP8 (can handle ~2x tokens vs BF16)

stage:
  name: "wikipedia_foundation_fp8"
  description: "FP8 curriculum: 64â†’2048 tokens with 2x larger batches"
  
data:
  dataset_name: "Yxanul/wikipedia-2k-high-quality"
  dataset_split: "train"
  streaming: false
  
  max_sequence_length: 2048
  stride: 1024
  
  tokenizer: "gpt2"
  group_by_length: false
  
training:
  # Duration
  num_epochs: 1
  max_steps: 100000
  
  # Base configuration
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  
  # Learning rate (FP8 can handle higher LRs)
  learning_rate: 8e-4      # Higher base LR for FP8
  min_learning_rate: 8e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  
  # FP8-Optimized Curriculum (~32-40k tokens per batch!)
  # Note: train_fp8.py will add 30% more to these batch sizes
  use_curriculum: true
  curriculum_stages:
    # Phase 1: Token relationships (massive batches with FP8)
    - {step: 0,     seq_len: 64,   batch_size: 512, lr_scale: 15.0, warmup: 500, grad_clip: 5.0}  # 32,768 tokens
    - {step: 2000,  seq_len: 128,  batch_size: 256, lr_scale: 10.0, warmup: 300, grad_clip: 2.0}  # 32,768 tokens
    - {step: 4000,  seq_len: 256,  batch_size: 128, lr_scale: 6.0,  warmup: 200, grad_clip: 1.0}  # 32,768 tokens
    
    # Phase 2: Sentence structure
    - {step: 8000,  seq_len: 512,  batch_size: 64,  lr_scale: 3.0,  warmup: 100, grad_clip: 0.5}  # 32,768 tokens
    - {step: 12000, seq_len: 768,  batch_size: 42,  lr_scale: 2.0,  warmup: 100, grad_clip: 0.4}  # 32,256 tokens
    
    # Phase 3: Document understanding
    - {step: 18000, seq_len: 1024, batch_size: 32,  lr_scale: 1.2,  warmup: 100, grad_clip: 0.3}  # 32,768 tokens
    - {step: 25000, seq_len: 1536, batch_size: 21,  lr_scale: 0.8,  warmup: 100, grad_clip: 0.3}  # 32,256 tokens
    
    # Phase 4: Deep mastery with FP8
    - {step: 35000, seq_len: 2048, batch_size: 16,  lr_scale: 0.6,  warmup: 100, grad_clip: 0.3}  # 32,768 tokens
    - {step: 50000, seq_len: 2048, batch_size: 18,  lr_scale: 0.4,  warmup: 50,  grad_clip: 0.3}  # 36,864 tokens
  
  # FP8 memory calculation (RTX 4090 24GB):
  # Base: ~32,768 tokens per batch
  # With FP8 +30% bonus: ~42,598 tokens per batch
  # Memory: ~11-12GB (FP8 uses ~50% of FP32 memory)
  # Leaves plenty of headroom for optimizer states
  
  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  weight_decay: 0.1
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # FP8 settings (handled by Transformer Engine)
  fp8_enabled: true
  fp8_margin: 0
  fp8_interval: 1
  fp8_amax_history: 16
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 10000
  save_total_limit: 3
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 2000
  
  # Logging
  logging_steps: 100
  logging_first_step: true
  report_to: ["wandb"]
  
  # Seed
  seed: 42
  
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 32  # Also larger for FP8
  
expected_results:
  # With FP8 + larger batches
  steps_to_perplexity_100: "~10,000"  # Even faster!
  steps_to_perplexity_50: "~20,000"
  steps_to_perplexity_30: "~32,000"
  
  # Speed improvements over BF16
  examples_seen_at_10k_steps: "4,000,000+"  # 2x more than BF16
  wall_clock_speedup: "5-6x vs baseline"  # 2x from FP8, 2-3x from curriculum
  fp8_speedup_over_bf16: "2x"
  
  # Per stage metrics with FP8
  stage_64_tokens_per_sec: "~40,000+"   # With FP8
  stage_256_tokens_per_sec: "~35,000"
  stage_512_tokens_per_sec: "~30,000"
  stage_1024_tokens_per_sec: "~25,000"
  stage_2048_tokens_per_sec: "~20,000"
  
memory_and_performance:
  - "FP8 uses E4M3 format for forward, E5M2 for backward"
  - "~50% memory reduction vs FP32"
  - "2x throughput improvement vs BF16"
  - "Automatic loss scaling handled by Transformer Engine"
  - "RTX 4090 fully supports FP8 (sm_89)"
  
notes:
  - "Batch sizes 2x larger than BF16 config"
  - "train_fp8.py adds extra 30% to batch sizes"
  - "Effective: ~42k tokens/batch in early stages"
  - "Higher base LR (8e-4) works well with FP8"
  - "Monitor first 100 steps for stability"
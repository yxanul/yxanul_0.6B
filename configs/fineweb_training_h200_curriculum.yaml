# H200-Optimized Curriculum Learning Configuration for 270M Model
# Leveraging 141GB VRAM for massive batch sizes
# Target: Beat Gemma 270M (trained on 2T tokens) with only 10B tokens

data:
  dataset_name: "fineweb-edu-highest-quality-2025"  # Local dataset
  max_sequence_length: 2048
  tokenizer: "SuperBPE-t80k"  # 37.5% fewer tokens!

training:
  # Total tokens to process (with SuperBPE-t80k)
  # 10B text tokens = 6.25B actual tokens (37.5% reduction)
  max_steps: 300000
  save_steps: 10000
  eval_steps: 2000
  logging_steps: 100
  
  # FP8 settings
  use_fp8: true
  fp8_format: "e4m3"
  
  # H200-optimized curriculum (10 stages, massive batch sizes)
  use_curriculum: true
  curriculum_stages:
    # Stage 1: Single phrases (8 tokens)
    - step: 0
      seq_len: 8
      batch_size: 4096  # 8x larger than RTX 4090
      lr_scale: 0.3
      grad_clip: 0.5
      warmup_steps: 1000
      gradient_accumulation_steps: 1  # No need with huge batch
      description: "Learning basic phrases and word patterns"
      # 32,768 tokens per batch!
    
    # Stage 2: Complete sentences (16 tokens)
    - step: 3000
      seq_len: 16
      batch_size: 2048  # Still massive
      lr_scale: 0.4
      grad_clip: 0.5
      gradient_accumulation_steps: 1
      description: "Learning complete sentences and basic grammar"
      # 32,768 tokens per batch
    
    # Stage 3: Compound sentences (32 tokens)
    - step: 8000
      seq_len: 32
      batch_size: 1024
      lr_scale: 0.5
      grad_clip: 0.7
      gradient_accumulation_steps: 1
      description: "Learning paragraph structure and flow"
      # 32,768 tokens per batch
    
    # Stage 4: Short paragraphs (64 tokens)
    - step: 15000
      seq_len: 64
      batch_size: 512
      lr_scale: 0.6
      grad_clip: 0.8
      gradient_accumulation_steps: 1
      description: "Learning multi-sentence coherence"
      # 32,768 tokens per batch
    
    # Stage 5: Full paragraphs (128 tokens)
    - step: 30000
      seq_len: 128
      batch_size: 256
      lr_scale: 0.75
      grad_clip: 0.9
      gradient_accumulation_steps: 1
      description: "Learning extended context and topic development"
      # 32,768 tokens per batch
    
    # Stage 6: Multiple paragraphs (256 tokens)
    - step: 50000
      seq_len: 256
      batch_size: 128
      lr_scale: 0.85
      grad_clip: 1.0
      gradient_accumulation_steps: 1
      description: "Learning section-level coherence"
      # 32,768 tokens per batch
    
    # Stage 7: Article sections (512 tokens)
    - step: 80000
      seq_len: 512
      batch_size: 64
      lr_scale: 0.95
      grad_clip: 1.0
      gradient_accumulation_steps: 1
      description: "Learning document structure"
      # 32,768 tokens per batch
    
    # Stage 8: Half articles (768 tokens)
    - step: 120000
      seq_len: 768
      batch_size: 48  # Slightly larger batches possible
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 1
      description: "Learning extended reasoning"
      # 36,864 tokens per batch
    
    # Stage 9: Near-full articles (1536 tokens)
    - step: 170000
      seq_len: 1536
      batch_size: 24
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 1
      description: "Learning complete document understanding"
      # 36,864 tokens per batch
    
    # Stage 10: Full articles (2048 tokens)
    - step: 220000
      seq_len: 2048
      batch_size: 16
      lr_scale: 1.0
      grad_clip: 1.0
      gradient_accumulation_steps: 1
      description: "Maximum context window training"
      # 32,768 tokens per batch

  # Learning rate schedule
  learning_rate: 6e-4  # Standard for smaller models
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01
  
  # Gradient accumulation (not needed with H200's massive batches)
  gradient_accumulation_steps: 1
  
  # Mixed precision
  mixed_precision: "bf16"
  tf32: true
  
  # Memory optimization
  gradient_checkpointing: true
  use_flash_attention: true

# Validation settings
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 128  # Much larger for H200
  eval_strategy: "steps"
  metric_for_best_model: "perplexity"
  greater_is_better: false

# Logging
logging:
  report_to: ["console", "wandb"]
  wandb_project: "yxanul-270m-h200"
  wandb_run_name: "beat-gemma-10b-tokens"
  log_level: "info"

# Expected Performance on H200:
#
# Throughput by stage:
# - Stage 1-2 (8-16 tokens): 150-200k tokens/sec
# - Stage 3-4 (32-64 tokens): 250-350k tokens/sec  
# - Stage 5-6 (128-256 tokens): 400-500k tokens/sec
# - Stage 7-10 (512-2048 tokens): 300-400k tokens/sec
#
# Training Timeline (estimated):
# - Hour 0-0.5: Stages 1-3, basic patterns
# - Hour 0.5-1: Stages 4-5, sentence mastery
# - Hour 1-2: Stages 6-7, paragraph mastery
# - Hour 2-3: Stages 8-10, document understanding
#
# Total: 3 hours to complete training!
#
# Expected Results:
# - Perplexity < 30 (competitive with Gemma)
# - Faster inference due to GQA
# - Better on reasoning tasks due to depth
# - Proof that quality > quantity in training data
# Yxanul 0.6B Deep Transformer Configuration
# Architecture: Deep and narrow for better compositional learning

model:
  name: "yxanul-0.6B-deep"
  type: "deep_transformer"
  
  # Core dimensions (deep & narrow)
  hidden_size: 768           # Smaller d_model
  num_layers: 28             # Much deeper than typical 12
  num_attention_heads: 12    
  num_kv_heads: 2            # Grouped-Query Attention (6:1 ratio)
  intermediate_size: 2048    # Optimized for SwiGLU: 8/3 * hidden_size
  
  # Vocabulary and positions
  vocab_size: 200005         # SuperBPE t=80k tokenizer (37.5% fewer tokens!)
  max_position_embeddings: 4096
  type_vocab_size: 0         # No token types
  
  # Advanced architecture features
  activation_function: "swiglu"  # Faster than GEGLU, used in Llama
  use_swiglu: true              # Enable SwiGLU
  use_rotary_embeddings: true   # RoPE for better length generalization
  use_rope: true                # Enable RoPE
  rope_theta: 10000.0
  rope_scaling: null           # Can add NTK scaling later
  
  # Normalization
  layer_norm_type: "rmsnorm"    # 10-15% faster than LayerNorm
  use_rmsnorm: true             # Enable RMSNorm
  pre_norm: true                # Pre-normalization (more stable)
  layer_norm_eps: 1e-5
  
  # Regularization (minimal for pretraining)
  attention_dropout: 0.0
  hidden_dropout: 0.0
  
  # Initialization
  initializer_range: 0.02      # Standard deviation
  use_scaled_init: true        # Scale by 1/sqrt(2*n_layers)
  
  # Memory optimizations
  use_gradient_checkpointing: true
  checkpoint_every_n_layers: 4   # Checkpoint every 4th layer
  
  # Attention optimizations
  use_flash_attention: true
  flash_attention_version: 3
  attention_implementation: "flash_attention_3"
  
  # Special tokens (SuperBPE uses different IDs)
  pad_token_id: 200004
  bos_token_id: 200004
  eos_token_id: 200004
  
  # Factorized embeddings (massive parameter savings!)
  use_factorized_embeddings: true
  factorization_dim: 128  # Bottleneck dimension (83% reduction in embedding params)
  
# Model size calculation (with SuperBPE-t80k + GQA + Factorized Embeddings)
# SuperBPE-t80k Factorized: vocab_size * r + r * hidden = 200005 * 128 + 128 * 768 = 25.7M
# Without factorization would be: 200005 * 768 = 153.6M (saved 127.9M params!)
# 
# Parameters per layer:
# Q_proj = hidden^2 = 768^2 = 590K  
# KV_proj = 2 * num_kv_heads * head_dim * hidden = 2 * 2 * 64 * 768 = 196K
# O_proj = hidden^2 = 590K
# FFN (SwiGLU) = 3 * hidden * intermediate = 3 * 768 * 2048 = 4.7M
# 
# Total = 25.7M (embeddings) + 28 * (590K + 196K + 590K + 4.7M)
#       = 25.7M + 28 * 6.1M
#       = 25.7M + 171M  
#       = 196.7M parameters (still under 200M!)
# 
# Benefits of SuperBPE-t80k (Research Mode):
# - 37.5% fewer tokens = 37.5% faster training!
# - Your 4.1B tokens â†’ 2.61B actual tokens processed
# - Chinchilla scaling: 1T tokens of text = 625B tokens compute
# - Factorized embeddings: 127.9M params saved
# - Switch to t=180k later for +0.4% better downstream performance
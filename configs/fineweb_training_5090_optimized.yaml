# Optimized curriculum for RTX 5090 (32GB) - Better GPU utilization
# Maintains learning benefits while avoiding extreme underutilization

model_name: "yxanul-270m"
model_type: "decoder-only"

data:
  max_sequence_length: 2048
  dataset_name: "Yxanul/experimental-pretrain-1b"
  dataset_path: "experimental-pretrain-1b"
  dataset_file: "dataset_1b.parquet"
  streaming: false
  use_mmap: true
  num_proc: 4
  pack_sequences: true  # Enable sequence packing for efficiency

training:
  num_epochs: 3
  warmup_steps: 2000
  max_steps: -1
  learning_rate: 1.6e-4  # Base LR
  min_learning_rate: 1.6e-5
  weight_decay: 0.1
  use_curriculum: true
  
  curriculum_stages:
    # Stage 1: Short documents (256 tokens) - Better GPU utilization with safe memory
    # With 200k vocab, must be conservative on batch size
    - step: 0
      seq_len: 256
      batch_size: 8  # Safe for 200k vocab (was 64 - too large!)
      lr_scale: 0.5
      grad_clip: 0.5
      warmup_steps: 1000
      gradient_accumulation_steps: 8  # 64 effective batch
      description: "Short documents and paragraphs"
      # 16,384 tokens per batch effective
    
    # Stage 2: Medium documents (512 tokens)
    - step: 5000
      seq_len: 512
      batch_size: 4
      lr_scale: 0.75
      grad_clip: 0.7
      gradient_accumulation_steps: 8  # 32 effective batch
      description: "Medium-length articles"
      # 16,384 tokens per batch effective
    
    # Stage 3: Standard documents (1024 tokens)
    - step: 15000
      seq_len: 1024
      batch_size: 2
      lr_scale: 1.0
      grad_clip: 0.9
      gradient_accumulation_steps: 8  # 16 effective batch
      description: "Full articles and stories"
      # 16,384 tokens per batch effective
    
    # Stage 4: Long documents (1536 tokens)
    - step: 30000
      seq_len: 1536
      batch_size: 1
      lr_scale: 1.1
      grad_clip: 1.0
      gradient_accumulation_steps: 16  # 16 effective batch
      description: "Long-form content"
      # 24,576 tokens per batch effective
    
    # Stage 5: Full context (2048 tokens)
    - step: 50000
      seq_len: 2048
      batch_size: 1
      lr_scale: 1.2
      grad_clip: 1.0
      gradient_accumulation_steps: 16  # 16 effective batch
      description: "Maximum context window"
      # 32,768 tokens per batch effective

  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  mixed_precision: "bf16"
  tf32: true
  
  # FP8 settings
  use_fp8: true
  fp8_margin: 0
  fp8_interval: 1
  fp8_format: "hybrid"  # E4M3 forward, E5M2 backward
  fp8_amax_history_len: 16
  fp8_amax_compute_algo: "max"
  fp8_calibration_steps: 300  # BF16 warmup after curriculum changes

validation:
  validation_split: 0.05
  per_device_eval_batch_size: 1  # Keep small for 200k vocab
  eval_steps: 2000
  eval_strategy: "steps"
  metric_for_best_model: "perplexity"
  greater_is_better: false
  load_best_model_at_end: true

logging:
  logging_steps: 100
  save_steps: 5000
  save_total_limit: 3
  report_to: ["wandb"]
  run_name: "yxanul-270m-5090-optimized"
  
checkpointing:
  save_strategy: "steps"
  save_steps: 5000
  save_total_limit: 3
  save_best_model: true
  resume_from_checkpoint: null

optimization:
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.98  # More stable for curriculum
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  
  # DeepSpeed config (optional)
  deepspeed: null
  
  # Torch compile (disabled for FP8)
  torch_compile: false
  torch_compile_backend: "inductor"
  torch_compile_mode: "default"

# Hardware optimizations
cuda_amp: true
dataloader_pin_memory: true
dataloader_num_workers: 2
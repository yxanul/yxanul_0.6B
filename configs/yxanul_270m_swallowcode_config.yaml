# Yxanul 270M with SwallowCode Integration
# Leveraging the world's best Python dataset

datasets:
  fineweb:
    name: "fineweb-edu-highest-quality-2025"
    tokens: 4.1B  # Available
    quality: "Top 7% educational content"
    
  math:
    name: "custom-math-dataset"
    tokens: 500M  # Your premium dataset
    quality: "Curated with solutions"
    
  swallowcode:
    name: "tokyotech-llm/swallow-code"
    tokens: 16.1B  # Total available
    tokens_to_use: 300M  # For 270M model
    quality: "SOTA - 50% better than Stack v2"
    features:
      - "100% syntax validated"
      - "Pylint style conformity"
      - "LLM-rewritten by Llama-3.3-70B"
      - "Self-contained examples"
      - "Professional patterns"

# Progressive Curriculum with SwallowCode
curriculum_stages:
  
  # Early stages - minimal code exposure
  - stage: 1-3
    tokens: 60M
    mix:
      fineweb: 0.92
      math: 0.08
      swallowcode: 0.00  # No code yet - build language first
    rationale: "Establish strong language foundation"
  
  # Introduce simple code patterns
  - stage: 4-5
    tokens: 115M
    mix:
      fineweb: 0.75
      math: 0.15
      swallowcode: 0.10  # Simple functions, type hints
    code_focus:
      - "Variable declarations with types"
      - "Simple functions with docstrings"
      - "Basic control flow"
  
  # Develop programming skills
  - stage: 6-7
    tokens: 225M
    mix:
      fineweb: 0.65
      math: 0.18
      swallowcode: 0.17  # More complex patterns
    code_focus:
      - "Classes and dataclasses"
      - "Error handling"
      - "List comprehensions"
      - "Decorators"
  
  # Advanced integration
  - stage: 8-9
    tokens: 350M
    mix:
      fineweb: 0.58
      math: 0.22
      swallowcode: 0.20  # Algorithms and systems
    code_focus:
      - "Complex algorithms (MDP, neural nets)"
      - "API interactions"
      - "Async patterns"
      - "Testing patterns"
  
  # Final mastery
  - stage: 10
    tokens: 250M
    mix:
      fineweb: 0.50
      math: 0.25
      swallowcode: 0.25  # Maximum code exposure
    code_focus:
      - "Full applications"
      - "Performance optimization"
      - "Design patterns"
      - "Production-ready code"

# Why SwallowCode Changes Everything
swallowcode_advantages:
  vs_stack_v2:
    - "50% better performance on benchmarks"
    - "Every function has type hints"
    - "Every function has docstrings"
    - "No syntax errors (100% validated)"
    - "Algorithmically optimized"
    
  vs_codeparrot:
    - "Consistent quality (no degradation)"
    - "Professional patterns only"
    - "Self-contained (no broken imports)"
    - "Real-world algorithms"
    
  unique_features:
    - "SGCR: Style-Guided Code Rewriting"
    - "SCOR: Self-Contained Optimization Rewriting"
    - "Rewritten by Llama-3.3-70B for clarity"
    - "16.1B tokens of pure quality"

# Expected Impact on Yxanul
performance_projections:
  code_generation:
    baseline: "30-40%"  # With Stack v2
    with_swallowcode: "60-75%"  # Conservative estimate
    improvement: "2x better"
    
  type_correctness:
    baseline: "20%"  # Typical small model
    with_swallowcode: "70%+"  # Learns from perfect examples
    
  documentation_quality:
    baseline: "Minimal"
    with_swallowcode: "Professional"  # Every function documented
    
  vs_gemma3_270m:
    gemma3_code: "Basic templates only"
    yxanul_code: "Production-ready implementations"

# Training Optimization
training_notes:
  - "SwallowCode is pre-validated - no need for syntax checking"
  - "Can use larger code chunks (already self-contained)"
  - "Higher learning rate for code portions (cleaner signal)"
  - "No filtering needed - already premium quality"

# Sample Allocation (1B tokens total)
token_budget:
  fineweb: 600M  # 60% - language foundation
  math: 200M     # 20% - reasoning ability  
  swallowcode: 200M  # 20% - coding mastery
  
  rationale: |
    With SwallowCode's 50% quality advantage, 200M tokens
    provides equivalent learning to 300M tokens of Stack v2,
    while maintaining strong language and reasoning abilities.
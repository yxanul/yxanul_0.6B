# Yxanul 270M - Gemma Killer Configuration
# Optimized to beat Gemma3 270M with 6,000x fewer tokens

model:
  vocab_size: 200005  # SuperBPE-t80k (37.5% token reduction)
  hidden_size: 896    # Increased from 768
  intermediate_size: 2432  # ~2.7x hidden (optimal for SwiGLU)
  num_hidden_layers: 32    # Deeper for better reasoning
  num_attention_heads: 14  # More heads for complexity
  num_kv_heads: 2          # GQA 7:1 ratio (aggressive compression)
  factorization_dim: 128   # Factorized embeddings
  max_position_embeddings: 4096
  rope_theta: 10000.0
  use_fp8: true
  
  # Architecture advantages over Gemma3
  activation: "swiglu"     # 15% faster than GELU
  normalization: "rmsnorm" # 15% faster than LayerNorm
  attention: "flash3"      # 3x faster, 10x less memory
  
  # Training optimizations
  gradient_checkpointing: true
  use_cache: true
  tie_word_embeddings: true

# Model Statistics
# Total Parameters: ~270M
# - Embeddings: 25.6M (200k × 128 factorized)
# - Projection: 114k (128 × 896)
# - Attention: 32 × (3.2M for QKV + 0.8M for O) = 128M
# - FFN: 32 × (2.2M + 2.2M) = 140M
# - Norms: ~60k
# Total: ~268M parameters

# Advantages over Gemma3:
# 1. GQA 7:1 vs probably MHA - 85% memory savings on KV cache
# 2. Factorized embeddings - 100M+ parameter savings
# 3. SwiGLU activation - smoother gradients
# 4. RoPE - better length generalization
# 5. Deeper (32 vs probably 24 layers) - better reasoning
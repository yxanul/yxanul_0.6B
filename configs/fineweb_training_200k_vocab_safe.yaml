# Ultra-conservative config for 200k vocabulary on 32GB GPU
# Prioritizes stability and avoiding OOM over speed

model_name: "yxanul-270m"
model_type: "decoder-only"

data:
  max_sequence_length: 1024  # Reduced from 2048 to save memory
  dataset_name: "Yxanul/experimental-pretrain-1b"
  dataset_path: "experimental-pretrain-1b"
  dataset_file: "dataset_1b.parquet"
  streaming: false
  use_mmap: true
  num_proc: 1  # Single process to save memory
  pack_sequences: false

training:
  num_epochs: 3
  warmup_steps: 2000
  max_steps: -1
  learning_rate: 1.6e-4
  min_learning_rate: 1.6e-5
  weight_decay: 0.1
  use_curriculum: false  # No curriculum to avoid reloading
  
  # Ultra-conservative batch settings for 200k vocab
  per_device_train_batch_size: 1  # Absolute minimum
  gradient_accumulation_steps: 32  # Effective batch = 32
  gradient_checkpointing: true  # Essential for memory
  mixed_precision: "bf16"
  tf32: true
  
  # DISABLE FP8 
  use_fp8: false
  
  # Optimizer settings for stability
  max_grad_norm: 0.5  # Aggressive clipping for stability
  
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 1  # Must be 1
  eval_steps: 5000  # Less frequent to save time
  eval_strategy: "steps"
  metric_for_best_model: "perplexity"
  greater_is_better: false
  load_best_model_at_end: false  # Save memory

logging:
  logging_steps: 100
  save_steps: 10000  # Less frequent saves
  save_total_limit: 2  # Keep only 2 checkpoints
  report_to: ["wandb"]
  run_name: "yxanul-270m-200k-vocab-safe"
  
checkpointing:
  save_strategy: "steps"
  save_steps: 10000
  save_total_limit: 2
  save_best_model: true
  resume_from_checkpoint: null

optimization:
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1e-8
  max_grad_norm: 0.5
  lr_scheduler_type: "cosine"
  
  # No DeepSpeed or torch.compile
  deepspeed: null
  torch_compile: false

# Memory optimizations
cuda_amp: true
dataloader_pin_memory: false
dataloader_num_workers: 0
gradient_checkpointing: true

# Environment variables to set before running:
# export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
# export CUDA_LAUNCH_BLOCKING=0
# export TOKENIZERS_PARALLELISM=false
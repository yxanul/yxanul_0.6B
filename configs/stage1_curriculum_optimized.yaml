# Optimized Curriculum Learning Configuration
# Based on Microsoft DeepSpeed's 3.3x speedup approach

stage:
  name: "wikipedia_foundation_curriculum"
  description: "Aggressive curriculum learning: 64→1024 tokens"
  
data:
  dataset_name: "Yxanul/wikipedia-2k-high-quality"
  dataset_split: "train"
  streaming: false  # Use local downloaded data
  
  # Start with 1024 max for faster training
  max_sequence_length: 1024  # Not 2048!
  stride: 512  # 50% overlap
  
  tokenizer: "gpt2"
  group_by_length: false  # Disable for curriculum
  
training:
  # Duration
  num_epochs: 1
  max_steps: 100000  # Fixed steps for fair comparison
  
  # Base configuration (will be overridden by curriculum)
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  
  # Learning rate (scales with effective batch size)
  learning_rate: 6e-4      # Base LR (will be scaled)
  min_learning_rate: 6e-5  # 10% of peak
  lr_scheduler_type: "cosine"
  warmup_steps: 500  # Adaptive per stage
  
  # Aggressive Curriculum (Microsoft-inspired + 4x LR)
  use_curriculum: true
  curriculum_stages:
    # Phase 1: Token relationships (8k steps, 2M examples!)
    - {step: 0,     seq_len: 64,   batch_size: 256, lr_scale: 11.3, warmup: 500, grad_clip: 5.0}
    - {step: 2000,  seq_len: 128,  batch_size: 128, lr_scale: 7.0,  warmup: 300, grad_clip: 2.0}
    - {step: 4000,  seq_len: 256,  batch_size: 64,  lr_scale: 3.5,  warmup: 200, grad_clip: 1.0}
    
    # Phase 2: Sentence structure
    - {step: 8000,  seq_len: 512,  batch_size: 32,  lr_scale: 1.5,  warmup: 100, grad_clip: 0.5}
    - {step: 12000, seq_len: 768,  batch_size: 21,  lr_scale: 1.0,  warmup: 100, grad_clip: 0.4}
    
    # Phase 3: Document understanding  
    - {step: 18000, seq_len: 1024, batch_size: 16,  lr_scale: 0.7,  warmup: 100, grad_clip: 0.3}
    - {step: 25000, seq_len: 1536, batch_size: 10,  lr_scale: 0.5,  warmup: 100, grad_clip: 0.3}
    
    # Phase 4: Deep mastery
    - {step: 35000, seq_len: 2048, batch_size: 8,   lr_scale: 0.4,  warmup: 100, grad_clip: 0.3}
    - {step: 50000, seq_len: 2048, batch_size: 8,   lr_scale: 0.3,  warmup: 50,  grad_clip: 0.3}
  
  # Memory calculation (RTX 4090 24GB):
  # All stages: ~16,384 tokens per batch
  # Memory: batch_size * seq_len * 768 * 4 bytes ≈ 12GB peak
  
  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  weight_decay: 0.1
  
  # Gradient clipping
  max_grad_norm: 1.0  # Less aggressive than before
  
  # Mixed precision
  fp16: false
  bf16: true
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 10000
  save_total_limit: 3
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 2000
  
  # Logging
  logging_steps: 100
  logging_first_step: true
  report_to: ["wandb"]
  
  # Seed
  seed: 42
  
validation:
  validation_split: 0.05
  per_device_eval_batch_size: 16
  
expected_results:
  # With aggressive curriculum
  steps_to_perplexity_100: "~15,000"  # Was ~30,000
  steps_to_perplexity_50: "~25,000"   # Was ~50,000
  steps_to_perplexity_30: "~40,000"   # Was ~80,000
  
  # Speed improvements
  examples_seen_at_10k_steps: "2,000,000"  # Was 320,000
  wall_clock_speedup: "2.5-3x"
  
  # Per stage metrics
  stage_64_tokens_per_sec: "~16,000"
  stage_256_tokens_per_sec: "~14,000"
  stage_512_tokens_per_sec: "~12,000"
  stage_1024_tokens_per_sec: "~10,000"
  
notes:
  - "Start ultra-short (64) to learn token relationships fast"
  - "Constant 16k tokens/batch maintains GPU efficiency"
  - "See 6x more examples in first 10k steps"
  - "LR scaling prevents instability with large batch changes"
  - "Stop at 1024 for most use cases (2048 optional)"
# Stage 1: Wikipedia Foundation Training
# Goal: Learn world knowledge and language structure

stage:
  name: "wikipedia_foundation"
  description: "Train on high-quality Wikipedia articles (2000+ tokens)"
  
data:
  # Primary dataset - YOUR uploaded Wikipedia dataset
  dataset_name: "Yxanul/wikipedia-2k-high-quality"
  dataset_split: "train"
  streaming: true          # Stream from HuggingFace
  num_examples: 239000     # Actual number of articles
  avg_tokens: 4000         # Average ~4K tokens per article
  total_tokens: 956000000  # ~1B tokens total
  
  # Data processing
  tokenizer: "gpt2"
  max_sequence_length: 2048
  stride: 1024             # 50% overlap for sliding window
  
  # Dynamic batching by sequence length
  group_by_length: true
  length_bins: [512, 1024, 1536, 2048]
  
  # Data augmentation
  random_truncate: false   # Keep full articles
  add_special_tokens: true
  
training:
  # Duration
  num_epochs: 15           # Deep learning on quality data
  max_steps: -1            # Train for full epochs
  
  # Batch configuration
  per_device_train_batch_size: 64  # Per GPU
  gradient_accumulation_steps: 2    # Effective batch = 64 * 8 * 2 = 1024
  
  # Learning rate
  learning_rate: 2e-4      # Peak LR
  min_learning_rate: 2e-5  # 10% of peak
  lr_scheduler_type: "linear_with_warmup"
  warmup_steps: 2000       # ~2% of total
  
  # Sequence length curriculum
  use_curriculum: true
  curriculum_stages:
    - {step: 0,     seq_len: 256,  batch_size: 128}
    - {step: 5000,  seq_len: 512,  batch_size: 96}
    - {step: 10000, seq_len: 1024, batch_size: 64}
    - {step: 20000, seq_len: 2048, batch_size: 32}
  
  # Optimization
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95        # Lower than default 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.1
  
  # Gradient clipping
  max_grad_norm: 0.3       # Aggressive clipping for stability
  
  # Mixed precision
  fp16: false
  bf16: true               # Better for training stability
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 5000
  save_total_limit: 5      # Keep last 5 checkpoints
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 1000
  eval_accumulation_steps: 4
  
  # Early stopping
  load_best_model_at_end: true
  metric_for_best_model: "eval_perplexity"
  greater_is_better: false
  early_stopping_patience: 10
  early_stopping_threshold: 0.001
  
  # Logging
  logging_steps: 100
  logging_first_step: true
  report_to: ["tensorboard", "wandb"]
  
  # Random seed
  seed: 42
  data_seed: 42
  
validation:
  # Held-out Wikipedia articles
  validation_split: 0.01   # 1% for validation
  
  # Metrics to track
  metrics:
    - perplexity
    - loss
    - gradient_norm
    - learning_rate
    
  # Zero-shot evaluation
  zero_shot_tasks:
    - task: "triviaqa"
      num_examples: 100
    - task: "hellaswag"  
      num_examples: 100
      
expected_results:
  # Target metrics after Stage 1
  perplexity: "< 10"       # Good language model
  triviaqa_accuracy: "> 0.25"  # Better than random
  hellaswag_accuracy: "> 0.35"  # Basic reasoning
  
  # Training speed (8x A100)
  tokens_per_second: 40000
  time_per_epoch: "0.4 hours"
  total_time: "6 hours"
  
notes:
  - "Wikipedia provides factual grounding"
  - "Long articles (2000+ tokens) teach relationships"
  - "15 epochs on quality > 5 epochs on quantity"
  - "Monitor for catastrophic forgetting in later stages"
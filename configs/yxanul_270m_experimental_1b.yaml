# Yxanul 270M Configuration for experimental-pretrain-1b dataset
# Optimized for TE v2.4 with curriculum learning

model:
  name: "Yxanul-270M-TEv2"
  vocab_size: 200005  # SuperBPE tokenizer
  hidden_size: 896
  intermediate_size: 2400  # ~2.67x hidden for SwiGLU
  num_hidden_layers: 32
  num_attention_heads: 14
  num_kv_heads: 2  # GQA with 7:1 ratio
  head_dim: 64
  max_position_embeddings: 4096
  rope_theta: 10000.0
  
  # TE v2.4 specific
  use_fp8: true
  params_dtype: "bfloat16"
  fuse_qkv_params: true  # Fused QKV for better memory access
  use_flash_attention: true  # Will use FA3 on NGC 25.05+
  
  # Factorized embeddings (Gemma-style)
  use_factorized_embedding: true
  factorization_dim: 128
  
  # Regularization
  hidden_dropout: 0.0  # No dropout for small model
  attention_dropout: 0.0
  
  # Normalization
  layernorm_epsilon: 1.0e-6
  zero_centered_gamma: false  # Standard RMSNorm

training:
  # Dataset configuration for experimental-pretrain-1b
  dataset: "Yxanul/experimental-pretrain-1b"
  dataset_path: "experimental-pretrain-1b"  # Folder at root level
  dataset_file: "dataset_1b.parquet"  # File inside the folder
  tokenizer: "superbpe_t80k"  # Fast variant
  
  # Training parameters aligned with train_te_v2.py args - RTX 5090 (32GB)
  batch_size: 1  # Ultra conservative - single sample at 2048 seq_len
  gradient_accumulation_steps: 16  # Effective batch size of 16
  learning_rate: 6.0e-4  # Default from train_te_v2.py
  num_epochs: 1  # Single epoch as requested
  max_steps: -1  # Run full epochs
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # FP8 Configuration (TE v2.4)
  fp8:
    enabled: true
    format: "hybrid"  # Default from train_te_v2.py
    calibration_steps: 10
    amax_history_len: 16
    amax_compute_algo: "max"
    reduce_amax: true
    fp8_dpa: true  # FP8 attention on H100
  
  # Scheduler
  scheduler:
    type: "cosine"
    warmup_steps: 1000
    min_lr_ratio: 0.1
  
  # Evaluation aligned with train_te_v2.py
  eval_steps: 2000  # Default from train_te_v2.py
  multi_domain_eval_steps: 10000  # Default from train_te_v2.py
  save_steps: 10000  # Default from train_te_v2.py
  save_total_limit: 3  # Default from train_te_v2.py
  
  # Checkpointing
  checkpoint_dir: "checkpoints_te_v2"
  save_best_model: true
  metric_for_best_model: "val/perplexity"
  
  # Data loading
  max_length: 2048  # Default from train_te_v2.py
  num_workers: 4  # Default from train_te_v2.py
  
  # Curriculum learning configuration
  use_curriculum: true
  curriculum_stages:
    # Progressive sequence length curriculum for 1B tokens (1 epoch)
    # Conservative batch sizes for RTX 4090 (24GB VRAM)
    # Model + optimizer ~2.5GB, leaving ~21GB for activations
    
    - stage: 1
      name: "Token Level"
      seq_len: 16
      tokens: 20_000_000  # 2% of data
      batch_size: 256  # Very large batch for tiny sequences
      gradient_accumulation_steps: 1
      dataset_mix:
        fineweb: 0.70  # Already mixed in dataset
        math: 0.15
        code: 0.15
      estimated_vram_gb: 3.0
    
    - stage: 2
      name: "Phrase Level"
      seq_len: 32
      tokens: 30_000_000  # 3% of data
      batch_size: 128  # Safe: ~0.5GB VRAM
      gradient_accumulation_steps: 1
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 3.0
    
    - stage: 3
      name: "Short Sentences"
      seq_len: 64
      tokens: 40_000_000  # 4% of data
      batch_size: 64  # Safe: ~0.5GB VRAM
      gradient_accumulation_steps: 2  # Effective batch 128
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 3.0
    
    - stage: 4
      name: "Full Sentences"
      seq_len: 128
      tokens: 60_000_000  # 6% of data
      batch_size: 32  # Safe: ~0.5GB VRAM usage
      gradient_accumulation_steps: 4  # Effective batch 128
      dataset_mix:
        fineweb: 0.70  # Already mixed in dataset
        math: 0.15
        code: 0.15
      estimated_vram_gb: 3.5  # Model + batch
    
    - stage: 5
      name: "Paragraph Understanding"
      seq_len: 256
      tokens: 80_000_000  # 8% of data
      batch_size: 32  # Safe: ~2GB VRAM
      gradient_accumulation_steps: 2  # Effective batch 64
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 4.5
    
    - stage: 6
      name: "Extended Paragraphs"
      seq_len: 512
      tokens: 120_000_000  # 12% of data
      batch_size: 4  # Reduced for RTX 5090 with large vocab
      gradient_accumulation_steps: 8  # Effective batch 32
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 6.5
    
    - stage: 7
      name: "Document Structure"
      seq_len: 1024
      tokens: 150_000_000  # 15% of data
      batch_size: 2  # Reduced for RTX 5090 with large vocab
      gradient_accumulation_steps: 8  # Effective batch 16
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 10.5
    
    - stage: 8
      name: "Long Context Mastery"
      seq_len: 2048
      tokens: 500_000_000  # 50% of data - main training
      batch_size: 1  # Single sample only for RTX 5090 with 200k vocab
      gradient_accumulation_steps: 16  # Effective batch 16
      dataset_mix:
        fineweb: 0.70
        math: 0.15
        code: 0.15
      estimated_vram_gb: 25.0  # Safe margin under 32GB

# Performance expectations for 1 epoch
performance:
  rtx_5090:
    tokens_per_sec: 80000  # Blackwell architecture improvements
    peak_memory_gb: 25.0  # Stage 8 peak usage
    time_for_1_epoch_hours: 3.5  # 1B tokens / 80k tps / 3600
    safe_margin: "7GB headroom at peak usage"
  
  h100:
    tokens_per_sec: 150000  # Can use larger batches
    peak_memory_gb: 18.5  # Same model, but could use more
    time_for_1_epoch_hours: 1.85
    note: "Can 4-8x batch sizes for better throughput"
  
  # Data efficiency with experimental-pretrain-1b
  dataset_info:
    total_samples: "~500k documents"
    total_tokens: "1B tokens"
    avg_tokens_per_doc: "~2000"
    pre_shuffled: true  # seed=42
    pre_mixed: true  # 70% fineweb, 15% math, 15% code
    single_epoch: true  # Only training for 1 epoch

# Memory safety calculations (RTX 5090 - 32GB)
memory_breakdown:
  model_params: "470M * 2 bytes = 940MB"  # Actual model is 470M not 270M
  optimizer_states: "470M * 8 bytes = 3.76GB (AdamW)"
  fp8_overhead: "~500MB (scaling factors, stats)"
  base_usage: "~5GB total"
  
  activation_memory_per_sample:
    seq_128: "~16MB"
    seq_256: "~32MB"
    seq_512: "~64MB"
    seq_1024: "~128MB"
    seq_2048: "~256MB"
  
  safe_batch_sizes:
    seq_128: "64 (uses ~2GB)"
    seq_256: "32 (uses ~3GB)"
    seq_512: "16 (uses ~5GB)"
    seq_1024: "4 (uses ~10GB)"
    seq_2048: "2 (uses ~20GB)"  # With 200k vocab, logits are huge

notes:
  - "1 EPOCH ONLY - processes each sample exactly once"
  - "Dataset at ./experimental-pretrain-1b/dataset_1b.parquet"
  - "Ultra-conservative batch sizes for RTX 5090 (32GB)"
  - "Heavy gradient accumulation for effective larger batches"
  - "Peak VRAM usage: ~25GB (7GB safety margin)"
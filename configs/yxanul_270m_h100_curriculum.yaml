# Yxanul 270M - H100 80GB SXM Optimized Curriculum
# Leveraging 6x compute power vs RTX 4090
# Expected: 350-400k tokens/sec with FP8

hardware:
  device: "H100-SXM-80GB"
  capabilities:
    fp16_tflops: 1979
    fp8_tflops: 3958
    memory_gb: 80
    memory_bandwidth_tb: 3.35
    tensor_cores: 528
    sm_count: 132
    
training:
  # H100 can handle much larger batches efficiently
  # Model memory: ~2.5GB in FP8, leaving 77GB for batches!
  
  total_tokens: 1_000_000_000  # 1B tokens total
  expected_throughput: 350_000  # tokens/sec (conservative)
  estimated_time_hours: 0.8     # 48 minutes!
  
  # Optimized for H100's massive compute
  curriculum_stages:
    
    # === PHASE 1: FOUNDATION (0-100M tokens) ===
    # H100 can handle massive batches at short sequences
    
    - stage: 1
      name: "Basic Patterns"
      step: 0
      seq_len: 8
      tokens: 10_000_000
      batch_size: 8192  # 16x larger than RTX 4090!
      gradient_accumulation_steps: 1  # No accumulation needed
      dataset_mix:
        fineweb: 0.95
        math: 0.05
        code: 0.00
      memory_usage_gb: 0.5
      expected_tokens_per_sec: 500_000  # Blazing fast
    
    - stage: 2
      name: "Sentences"
      step: 10_000
      seq_len: 16
      tokens: 20_000_000
      batch_size: 4096
      dataset_mix:
        fineweb: 0.95
        math: 0.05
        code: 0.00
      memory_usage_gb: 1.0
      expected_tokens_per_sec: 450_000
    
    - stage: 3
      name: "Paragraphs"
      step: 30_000
      seq_len: 32
      tokens: 30_000_000
      batch_size: 2048
      dataset_mix:
        fineweb: 0.90
        math: 0.08
        code: 0.02
      memory_usage_gb: 2.0
      expected_tokens_per_sec: 420_000
    
    - stage: 4
      name: "Extended Context"
      step: 60_000
      seq_len: 64
      tokens: 40_000_000
      batch_size: 1024
      dataset_mix:
        fineweb: 0.85
        math: 0.10
        code: 0.05
      memory_usage_gb: 4.0
      expected_tokens_per_sec: 400_000
    
    # === PHASE 2: SKILL BUILDING (100-400M tokens) ===
    
    - stage: 5
      name: "Reasoning Development"
      step: 100_000
      seq_len: 128
      tokens: 75_000_000
      batch_size: 512
      dataset_mix:
        fineweb: 0.80
        math: 0.12
        code: 0.08
      memory_usage_gb: 8.0
      expected_tokens_per_sec: 380_000
    
    - stage: 6
      name: "Complex Patterns"
      step: 175_000
      seq_len: 256
      tokens: 100_000_000
      batch_size: 256
      dataset_mix:
        fineweb: 0.75
        math: 0.15
        code: 0.10
      memory_usage_gb: 16.0
      expected_tokens_per_sec: 360_000
    
    - stage: 7
      name: "Integration"
      step: 275_000
      seq_len: 512
      tokens: 125_000_000
      batch_size: 128
      dataset_mix:
        fineweb: 0.70
        math: 0.18
        code: 0.12
      memory_usage_gb: 32.0
      expected_tokens_per_sec: 340_000
    
    # === PHASE 3: ADVANCED (400-750M tokens) ===
    
    - stage: 8
      name: "Deep Understanding"
      step: 400_000
      seq_len: 768
      tokens: 150_000_000
      batch_size: 96   # Still massive for this seq length
      dataset_mix:
        fineweb: 0.65
        math: 0.20
        code: 0.15
      memory_usage_gb: 45.0
      expected_tokens_per_sec: 320_000
    
    - stage: 9
      name: "Expert Reasoning"
      step: 550_000
      seq_len: 1536
      tokens: 200_000_000
      batch_size: 48
      dataset_mix:
        fineweb: 0.60
        math: 0.22
        code: 0.18
      memory_usage_gb: 55.0
      expected_tokens_per_sec: 300_000
    
    # === PHASE 4: MASTERY (750M-1B tokens) ===
    
    - stage: 10
      name: "Final Polish"
      step: 750_000
      seq_len: 2048
      tokens: 250_000_000
      batch_size: 40   # Still 10x larger than RTX 4090!
      dataset_mix:
        fineweb: 0.55
        math: 0.25
        code: 0.20
      memory_usage_gb: 65.0
      expected_tokens_per_sec: 280_000
  
  # H100-optimized settings
  optimizer: "adamw"
  learning_rate: 8e-4  # Can be slightly higher with larger batches
  weight_decay: 0.1
  gradient_clipping: 1.0
  
  # Mixed precision
  mixed_precision: "bf16"
  use_fp8: true
  fp8_format: "e4m3"  # H100 native format
  
  # H100 specific optimizations
  use_flash_attention: true
  use_xformers: true  # H100 optimized kernels
  compile_mode: "max-autotune"  # PyTorch 2.0 compile
  enable_tensorrt: false  # Not needed for training
  
  # Multi-GPU ready (if using multiple H100s)
  distributed_strategy: "fsdp"  # Fully Sharded Data Parallel
  fsdp_sharding_strategy: "full_shard"
  
# SFT Phase - Even faster on H100
sft_phase:
  enabled: true
  start_after_tokens: 1_000_000_000
  training_tokens: 100_000_000
  batch_size: 32  # 8x larger than typical
  learning_rate: 3e-5
  expected_time_minutes: 5  # Just 5 minutes for SFT!

# Performance Expectations
performance:
  h100_vs_4090:
    pretraining_speedup: "6x"
    tokens_per_second:
      h100: "350,000"
      rtx_4090: "60,000"
    
  h100_vs_h200:
    compute_difference: "~same"
    memory_utilization:
      h100_80gb: "65GB peak (81% utilized)"
      h200_141gb: "65GB peak (46% utilized - wasteful)"
    cost_efficiency:
      h100: "Optimal - using 81% of capacity"
      h200: "Wasteful - paying for unused VRAM"
    
  training_time:
    pretrain_1b_tokens: "48 minutes"
    sft_100m_tokens: "5 minutes"
    total: "53 minutes"
    
  estimated_cost:
    h100_hourly: "$2.50"
    total_cost: "$2.21"  # Less than a coffee!
    vs_gemma3: ">50,000x cheaper"

# Batch Size Rationale for H100
batch_size_calculation:
  model_params: "270M"
  optimizer_states: "540M (2x for Adam)"
  activations_per_sample: "~50MB at 2048 seq_len"
  
  available_memory: "77GB (after model/optimizer)"
  
  max_batch_sizes:
    seq_8: "8192 (using 25GB)"
    seq_16: "4096 (using 25GB)"
    seq_32: "2048 (using 25GB)"
    seq_64: "1024 (using 25GB)"
    seq_128: "512 (using 25GB)"
    seq_256: "256 (using 25GB)"
    seq_512: "128 (using 25GB)"
    seq_1024: "64 (using 25GB)"
    seq_2048: "40 (using 65GB)"
    
  note: "Conservative estimates leaving headroom for PyTorch overhead"

# Why H100 80GB is Perfect
why_h100_not_h200:
  - "270M model doesn't need 141GB VRAM"
  - "H100 80GB already enables massive batches"
  - "Same compute performance as H200"
  - "30-50% cheaper per hour"
  - "Better memory bandwidth utilization ratio"
  - "Faster allocation on cloud providers"
#!/usr/bin/env python3
"""
FP8-Optimized Training Script with Transformer Engine
Provides 2x throughput on RTX 4090 with automatic FP8 mixed precision.
"""

import os
import sys
import glob
import time
from pathlib import Path

# Disable WandB's automatic torch hooks before importing torch
os.environ["WANDB_DISABLE_SERVICE"] = "true"
os.environ["WANDB_REQUIRE_SERVICE"] = "false"
os.environ["WANDB_WATCH_DISABLED"] = "true"  # Disable model watching that conflicts with torch.compile

# Enable memory optimization for better CUDA allocation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import torch._dynamo
torch._dynamo.config.suppress_errors = True

# Completely disable WandB's automatic PyTorch integration
try:
    import wandb
    # Disable all automatic hooks
    wandb.errors.term._silent = True  # Silence wandb warnings
except ImportError:
    pass  # WandB not installed
os.environ["WANDB_SILENT"] = "true"

import yaml
import argparse
from dataclasses import fields, asdict

# Add src to path
sys.path.append('src')

# Import FP8-optimized components
from model_fp8_optimized import create_fp8_model, ModelConfig
from trainer_fp8 import FP8Trainer
from data_pipeline import create_dataloader, create_tokenizer
from checkpoint_manager import CheckpointManager

# Check for Transformer Engine
try:
    import transformer_engine.pytorch as te
    TRANSFORMER_ENGINE_AVAILABLE = True
    print("Transformer Engine: Loaded successfully")
    print("FP8 support: ENABLED")
except ImportError:
    TRANSFORMER_ENGINE_AVAILABLE = False
    print("WARNING: Transformer Engine not installed!")
    print("Falling back to BF16 training...")


class CurriculumManager:
    """Manages curriculum learning stage transitions."""
    
    def __init__(self, config):
        self.config = config
        self.stages = config.get('training', {}).get('curriculum_stages', [])
        self.current_stage_idx = 0
        self.current_stage = self.stages[0] if self.stages else None
        
    def get_stage_for_step(self, global_step):
        """Get the curriculum stage for the current step."""
        if not self.stages:
            return None
            
        for i, stage in enumerate(self.stages):
            if global_step >= stage.get('step', 0):
                self.current_stage_idx = i
                self.current_stage = stage
            else:
                break
                
        return self.current_stage
    
    def should_update_dataloader(self, global_step, last_batch_size, last_seq_len=None):
        """Check if we need to recreate the dataloader."""
        stage = self.get_stage_for_step(global_step)
        if stage:
            # Check if batch size OR sequence length changed
            batch_changed = stage.get('batch_size') != last_batch_size
            seq_changed = last_seq_len is not None and stage.get('seq_len') != last_seq_len
            if batch_changed or seq_changed:
                return True, stage
        return False, stage
    
    def get_lr_scale(self, global_step):
        """Get learning rate scale for current stage."""
        stage = self.get_stage_for_step(global_step)
        return stage.get('lr_scale', 1.0) if stage else 1.0
    
    def get_grad_clip(self, global_step):
        """Get gradient clipping value for current stage."""
        stage = self.get_stage_for_step(global_step)
        return stage.get('grad_clip', 1.0) if stage else 1.0


def main():
    parser = argparse.ArgumentParser(description='Train Yxanul with FP8 Optimization')
    parser.add_argument('--config', type=str, required=True,
                        help='Path to training config file')
    parser.add_argument('--model-config', type=str, default='configs/model_config.yaml',
                        help='Path to model config file')
    parser.add_argument('--optimization-config', type=str, default='configs/optimization.yaml',
                        help='Path to optimization config file')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='Path to checkpoint to resume from')
    parser.add_argument('--local-rank', type=int, default=-1,
                        help='Local rank for distributed training')
    parser.add_argument('--eval-only', action='store_true',
                        help='Only run evaluation')
    parser.add_argument('--disable-fp8', action='store_true',
                        help='Disable FP8 training (use BF16 instead)')
    args = parser.parse_args()
    
    # Verify config file exists
    if not Path(args.config).exists():
        print(f"Error: Config file '{args.config}' not found!")
        print("Available configs:")
        for config in Path('configs').glob('*.yaml'):
            print(f"  - {config}")
        sys.exit(1)
    
    # Set device
    if args.local_rank == -1:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        world_size = 1
        rank = 0
    else:
        torch.cuda.set_device(args.local_rank)
        device = torch.device('cuda', args.local_rank)
        torch.distributed.init_process_group(backend='nccl')
        world_size = torch.distributed.get_world_size()
        rank = torch.distributed.get_rank()
    
    # Check GPU capabilities
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_capability = torch.cuda.get_device_capability(0)
        print(f"GPU: {gpu_name}")
        print(f"Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}")
        
        # RTX 4090 is sm_89 (8.9)
        if gpu_capability[0] == 8 and gpu_capability[1] == 9:
            print("RTX 4090 detected - FP8 training fully supported!")
        elif gpu_capability[0] >= 8:
            print("Ada/Hopper/Blackwell GPU detected - FP8 supported!")
        else:
            print("Warning: GPU may not support FP8 training efficiently")
    
    # Load configurations
    print(f"Loading config from: {args.config}")
    with open(args.config, 'r') as f:
        training_config = yaml.safe_load(f)
    
    # IGNORE model config from YAML - use hardcoded 270M config
    # This ensures we always get the correct model size
    from model_fp8_optimized import ModelConfig
    model_config_obj = ModelConfig()  # This has the correct 270M settings
    model_config_obj.use_fp8 = not args.disable_fp8
    
    # Convert to dict for compatibility
    model_config = {'model': asdict(model_config_obj)}
    
    # Load optimization config but disable torch.compile for FP8 models
    if Path(args.optimization_config).exists():
        with open(args.optimization_config, 'r') as f:
            optimization_config = yaml.safe_load(f)
    else:
        optimization_config = {}
    
    # DISABLE torch.compile - it's incompatible with our FP8 model
    if 'torch_compile' in optimization_config:
        optimization_config['torch_compile']['enabled'] = False
        print("NOTE: Disabled torch.compile (incompatible with FP8 model)")
    
    # Combine configs
    full_config = {
        'model': model_config,
        'training': training_config.get('training', {}),
        'optimization': optimization_config,
        'data': training_config.get('data', {}),
        'validation': training_config.get('validation', {})
    }
    
    # Initialize curriculum manager
    curriculum_mgr = CurriculumManager(training_config)
    use_curriculum = training_config.get('training', {}).get('use_curriculum', False)
    
    if rank == 0:
        print("=" * 60)
        print("Yxanul 270M Training with FP8 Optimization")
        print("=" * 60)
        print(f"Config: {args.config}")
        print(f"Device: {device}")
        print(f"FP8 Training: {'ENABLED' if not args.disable_fp8 else 'DISABLED (using BF16)'}")
        print(f"Curriculum Learning: {'ENABLED' if use_curriculum else 'DISABLED'}")
        if use_curriculum:
            print(f"Curriculum Stages: {len(curriculum_mgr.stages)}")
        print("=" * 60)
    
    # Get max sequence length from curriculum or config FIRST
    max_seq_len = training_config.get('data', {}).get('max_sequence_length', 2048)
    if use_curriculum and curriculum_mgr.stages:
        # Find the maximum seq_len across all curriculum stages
        max_seq_len = max(stage.get('seq_len', max_seq_len) for stage in curriculum_mgr.stages)
    
    # Create FP8-optimized model - use the config object directly
    model = create_fp8_model(model_config_obj)  # Pass the ModelConfig object
    model = model.to(device)
    
    # Verify curriculum doesn't exceed model's positional embedding capacity
    model_max_pos = model_config_obj.max_position_embeddings
    if use_curriculum and curriculum_mgr.stages:
        for i, stage in enumerate(curriculum_mgr.stages):
            stage_seq_len = stage.get('seq_len', 0)
            if stage_seq_len > model_max_pos:
                print(f"WARNING: Curriculum stage {i+1} seq_len ({stage_seq_len}) exceeds model's max_position_embeddings ({model_max_pos})")
                print(f"         Model has RoPE so it may extrapolate, but quality could degrade!")
    
    if max_seq_len > model_max_pos:
        print(f"NOTICE: Maximum sequence length ({max_seq_len}) exceeds model's trained position limit ({model_max_pos})")
        print(f"        RoPE can extrapolate but may have degraded performance beyond {model_max_pos} tokens")
    
    # Print expected performance
    if rank == 0 and not args.disable_fp8:
        print("\nExpected FP8 Performance Improvements:")
        print("  - 2x throughput vs BF16")
        print("  - 50% memory reduction vs FP32")
        print("  - Higher batch sizes possible")
        print("  - Automatic loss scaling")
        print("=" * 60)
    
    # Create tokenizer - Using SuperBPE for 31% token reduction!
    tokenizer = create_tokenizer(use_superbpe=True)  # Use SuperBPE t=80k
    tokenizer.model_max_length = max_seq_len  # Set max length after creation
    print(f"Tokenizer: SuperBPE with max_length={max_seq_len}")
    
    # Get initial batch size (from config or first curriculum stage)
    if use_curriculum and curriculum_mgr.stages:
        initial_batch_size = curriculum_mgr.stages[0].get('batch_size', 32)
        initial_seq_len = curriculum_mgr.stages[0].get('seq_len', 2048)
        # No FP8 bonus - we've already set conservative sizes
        print(f"Initial batch size: {initial_batch_size}, seq_len: {initial_seq_len}")
    else:
        initial_batch_size = training_config.get('training', {}).get('per_device_train_batch_size', 32)
        initial_seq_len = training_config.get('data', {}).get('max_sequence_length', 2048)
    
    # Create initial dataloaders with curriculum seq_len if applicable
    train_dataloader, train_dataset = create_dataloader(
        dataset_name=training_config.get('data', {}).get('dataset_name', 'Yxanul/wikipedia-2k-high-quality'),
        tokenizer=tokenizer,
        batch_size=initial_batch_size,
        max_length=initial_seq_len,  # Use curriculum seq_len from stage 1
        stage_config=training_config,
        num_workers=2,
        split='train'
    )
    
    # Validation dataloader
    val_dataloader = None
    val_split = training_config.get('validation', {}).get('validation_split', 0.05)
    if val_split > 0:
        val_batch_size = training_config.get('validation', {}).get('per_device_eval_batch_size', 16)
        if not args.disable_fp8:
            val_batch_size = int(val_batch_size * 1.3)  # Larger batches for validation too
        
        val_dataloader, _ = create_dataloader(
            dataset_name=training_config.get('data', {}).get('dataset_name'),
            tokenizer=tokenizer,
            batch_size=val_batch_size,
            max_length=training_config.get('data', {}).get('max_sequence_length', 2048),
            stage_config=training_config,
            num_workers=2,
            split=f'train[{int((1-val_split)*100)}%:]'
        )
    
    # Create FP8 trainer
    if rank == 0:
        print("\nInitializing FP8 Trainer...")
    
    trainer = FP8Trainer(
        model=model,
        config_path="configs",
        stage="fp8_curriculum_training",
        local_rank=args.local_rank,
        use_fp8=not args.disable_fp8
    )
    
    # Set trainer attributes
    trainer.train_dataloader = train_dataloader
    trainer.val_dataloader = val_dataloader
    trainer.tokenizer = tokenizer
    trainer.device = device
    trainer.world_size = world_size
    trainer.local_rank = rank if rank >= 0 else 0
    trainer.config = full_config
    
    # Load checkpoint if provided
    start_step = 0
    if args.checkpoint and os.path.exists(args.checkpoint):
        if rank == 0:
            print(f"\nLoading checkpoint from {args.checkpoint}...")
        checkpoint = trainer.load_checkpoint(args.checkpoint)
        start_step = checkpoint.get('global_step', 0)
        if rank == 0:
            print(f"Resumed from step {start_step}")
    
    # Training parameters (handle flat structure and ensure correct types)
    num_epochs = int(training_config.get('num_epochs', training_config.get('training', {}).get('num_epochs', 1)))
    max_steps = int(training_config.get('max_steps', training_config.get('training', {}).get('max_steps', -1)))
    checkpoint_steps = int(training_config.get('save_steps', training_config.get('training', {}).get('save_steps', 10000)))
    eval_steps = int(training_config.get('eval_steps', training_config.get('training', {}).get('eval_steps', 2000)))
    logging_steps = int(training_config.get('logging_steps', training_config.get('training', {}).get('logging_steps', 100)))
    save_total_limit = int(training_config.get('save_total_limit', training_config.get('training', {}).get('save_total_limit', 3)))
    
    # Get base learning rate (handle both nested and flat structure, ensure float)
    if 'learning_rate' in training_config:
        base_lr = float(training_config.get('learning_rate', 6e-4))
    else:
        base_lr = float(training_config.get('training', {}).get('learning_rate', 6e-4))
    
    # Initialize checkpoint manager
    checkpoint_manager = CheckpointManager(
        checkpoint_dir="checkpoints",
        max_checkpoints=save_total_limit,
        save_best=True,
        metric_name="val/perplexity",
        metric_mode="min"
    )
    trainer.checkpoint_manager = checkpoint_manager
    
    # Check for resuming from checkpoint
    resume_info = checkpoint_manager.get_resume_info()
    if resume_info and not args.eval_only:
        print(f"\nðŸ“‚ Found checkpoint to resume from: {resume_info['checkpoint_path']}")
        print(f"   Step: {resume_info['global_step']}, Epoch: {resume_info['epoch']}")
        # Note: Actual loading would happen here if implementing full resume
    
    # Initialize WandB
    if rank == 0:
        trainer._init_wandb()
    
    # Evaluation only mode
    if args.eval_only:
        if rank == 0:
            print("\nRunning evaluation only...")
            if val_dataloader:
                val_ppl = trainer.validate(val_dataloader)
                print(f"Validation perplexity: {val_ppl:.2f}")
            else:
                print("No validation dataloader available")
        return
    
    # Training loop
    if rank == 0:
        print("\n" + "=" * 60)
        print("Starting FP8 Training")
        print("=" * 60)
        print(f"Steps: {max_steps if max_steps > 0 else 'Until epochs complete'}")
        print(f"Initial batch size: {initial_batch_size}")
        print(f"Base learning rate: {base_lr}")
        print(f"FP8 enabled: {not args.disable_fp8}")
        print("=" * 60)
    
    global_step = start_step
    current_batch_size = initial_batch_size
    current_seq_len = training_config.get('data', {}).get('max_sequence_length', 2048)
    
    # Override with first curriculum stage if available
    if use_curriculum and curriculum_mgr.stages:
        current_seq_len = curriculum_mgr.stages[0].get('seq_len', current_seq_len)
    
    # Main training loop
    global_step = 0
    val_ppl = None  # Initialize validation perplexity for checkpoint tracking
    
    for epoch in range(num_epochs):
        if rank == 0:
            print(f"\n[Epoch {epoch + 1}/{num_epochs}]")
        
        model.train()
        epoch_loss = 0
        epoch_tokens = 0
        epoch_steps = 0
        
        # Create iterator
        train_iter = iter(train_dataloader)
        
        try:
            while True:
                # Check if we need to update dataloader for curriculum
                if use_curriculum:
                    should_update, stage = curriculum_mgr.should_update_dataloader(global_step, current_batch_size, current_seq_len)
                    
                    if should_update:
                        new_batch_size = stage.get('batch_size', current_batch_size)
                        # No FP8 bonus - using conservative sizes
                        new_seq_len = stage.get('seq_len', training_config.get('data', {}).get('max_sequence_length', 2048))
                        
                        if rank == 0:
                            print(f"\n[CURRICULUM UPDATE at step {global_step}]")
                            print(f"  Batch size: {current_batch_size} â†’ {new_batch_size}")
                            print(f"  Sequence length: {current_seq_len} â†’ {new_seq_len}")
                            print(f"  LR scale: {stage.get('lr_scale', 1.0)}x")
                            print(f"  Gradient clip: {stage.get('grad_clip', 1.0)}")
                            print(f"  Gradient accumulation: {stage.get('gradient_accumulation_steps', 1)} steps")
                        
                        # Recreate dataloader with new batch size AND sequence length!
                        train_dataloader, train_dataset = create_dataloader(
                            dataset_name=training_config.get('data', {}).get('dataset_name'),
                            tokenizer=tokenizer,
                            batch_size=new_batch_size,
                            max_length=new_seq_len,  # CRITICAL: Apply the curriculum sequence length!
                            stage_config=training_config,
                            num_workers=2,
                            split='train'
                        )
                        trainer.train_dataloader = train_dataloader
                        current_batch_size = new_batch_size
                        current_seq_len = new_seq_len
                        
                        # Update learning rate
                        lr_scale = float(stage.get('lr_scale', 1.0))
                        new_lr = base_lr * lr_scale
                        for param_group in trainer.optimizer.param_groups:
                            param_group['lr'] = new_lr
                        
                        # Update gradient clipping
                        trainer.max_grad_norm = stage.get('grad_clip', 1.0)
                        
                        # Create new iterator
                        train_iter = iter(train_dataloader)
                
                # Get next batch
                try:
                    batch = next(train_iter)
                except StopIteration:
                    break
                
                # Training step with FP8
                metrics = trainer.train_step(batch)
                
                epoch_loss += metrics['train/loss']
                epoch_tokens += metrics.get('data/actual_tokens', 0)
                epoch_steps += 1
                global_step += 1
                
                # Logging
                if global_step % logging_steps == 0 and rank == 0:
                    fp8_status = "FP8" if metrics.get('fp8/enabled', 0) > 0 else "BF16"
                    print(f"  Step {global_step}: Loss={metrics['train/loss']:.4f}, "
                          f"PPL={metrics['train/perplexity']:.2f}, "
                          f"LR={metrics['optim/learning_rate']:.2e}, "
                          f"Tokens/s={metrics['perf/tokens_per_second']:.0f} [{fp8_status}]")
                    
                    # Log curriculum info
                    if use_curriculum:
                        metrics['curriculum/stage'] = curriculum_mgr.current_stage_idx
                        metrics['curriculum/batch_size'] = current_batch_size
                    
                    # Log to WandB
                    try:
                        import wandb
                        wandb.log(metrics, step=global_step)
                    except:
                        pass
                
                # Validation
                if val_dataloader is not None and global_step % eval_steps == 0:
                    if rank == 0:
                        print(f"\n  Running validation at step {global_step}...")
                    val_ppl = trainer.validate(val_dataloader, max_batches=50)
                    if rank == 0 and val_ppl:
                        print(f"  Validation perplexity: {val_ppl:.2f}")
                    model.train()
                
                # Multi-domain validation (less frequent - every 5 normal validations)
                # This helps tune curriculum data ratios based on domain-specific performance
                if global_step % (eval_steps * 5) == 0 and global_step > 0:
                    if rank == 0:
                        print(f"\n  Running multi-domain validation at step {global_step}...")
                        print("  This measures perplexity across English, Math, and Code domains")
                    try:
                        multi_domain_results = trainer.validate_multi_domain(max_batches=50)
                        if rank == 0 and multi_domain_results:
                            print("  Multi-domain validation complete - check logs for detailed metrics")
                    except Exception as e:
                        if rank == 0:
                            print(f"  Multi-domain validation skipped: {e}")
                    model.train()
                
                # Checkpointing with automatic rotation and best model tracking
                if global_step % checkpoint_steps == 0 and rank == 0:
                    # Collect current metrics
                    checkpoint_metrics = {
                        'train/loss': metrics.get('loss', 0),
                        'train/perplexity': metrics.get('perplexity', 0),
                        'train/learning_rate': metrics.get('learning_rate', 0),
                    }
                    
                    # Add validation metrics if available
                    if val_ppl is not None:
                        checkpoint_metrics['val/perplexity'] = val_ppl
                    
                    # Save checkpoint using the manager
                    checkpoint_path = checkpoint_manager.save_checkpoint(
                        model=model,
                        optimizer=trainer.optimizer,
                        scheduler=trainer.scheduler,
                        epoch=epoch,
                        step=global_step,
                        metrics=checkpoint_metrics,
                        config={'training_config': training_config, 'model_config': asdict(model_config)},
                        scaler=trainer.scaler if hasattr(trainer, 'scaler') else None,
                        fp8_recipe=trainer.fp8_recipe if hasattr(trainer, 'fp8_recipe') and trainer.use_fp8 else None,
                        curriculum_stage=curriculum_mgr.current_stage_idx if use_curriculum else None,
                        dataset_tokens_seen=global_step * batch_size * max_length
                    )
                
                # Check max steps
                if max_steps > 0 and global_step >= max_steps:
                    if rank == 0:
                        print(f"\nReached max steps ({max_steps})")
                    break
                    
        except KeyboardInterrupt:
            if rank == 0:
                print("\nTraining interrupted by user")
            break
        
        # Epoch summary
        if rank == 0 and epoch_steps > 0:
            avg_loss = epoch_loss / epoch_steps
            avg_tokens_per_second = epoch_tokens / epoch_steps
            
            print(f"\n[Epoch {epoch + 1} Complete]")
            print(f"  Average loss: {avg_loss:.4f}")
            print(f"  Total tokens: {epoch_tokens:,}")
            print(f"  Avg tokens/second: {avg_tokens_per_second:.0f}")
            if not args.disable_fp8:
                print(f"  FP8 speedup: ~2x over BF16")
        
        # Check if we should stop
        if max_steps > 0 and global_step >= max_steps:
            break
    
    # Final checkpoint
    if rank == 0:
        # Collect final metrics
        final_metrics = {
            'train/final_loss': epoch_loss / epoch_steps if epoch_steps > 0 else 0,
            'train/final_perplexity': torch.exp(torch.tensor(epoch_loss / epoch_steps)).item() if epoch_steps > 0 else 0,
            'epochs_completed': num_epochs,
            'total_steps': global_step,
        }
        
        # Save final checkpoint with manager
        final_checkpoint = checkpoint_manager.save_checkpoint(
            model=model,
            optimizer=trainer.optimizer,
            scheduler=trainer.scheduler,
            epoch=num_epochs,
            step=global_step,
            metrics=final_metrics,
            config={'training_config': training_config, 'model_config': asdict(model_config)},
            scaler=trainer.scaler if hasattr(trainer, 'scaler') else None,
            fp8_recipe=trainer.fp8_recipe if hasattr(trainer, 'fp8_recipe') and trainer.use_fp8 else None,
            curriculum_stage=curriculum_mgr.current_stage_idx if use_curriculum else None,
            dataset_tokens_seen=global_step * batch_size * max_length,
            is_final=True
        )
        
        print(f"\n[Training Complete] Final model saved! Check checkpoints/ for best model.")
        
        # Print FP8 training summary
        if not args.disable_fp8:
            print("\nFP8 Training Summary:")
            print("  - Successfully trained with FP8 precision")
            print("  - Expected 2x speedup over BF16")
            print("  - Reduced memory usage by ~50%")
            print("  - Automatic loss scaling handled by Transformer Engine")
        
        # Close WandB
        try:
            import wandb
            wandb.finish()
        except:
            pass


if __name__ == "__main__":
    main()